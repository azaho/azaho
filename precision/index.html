<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <title>Andrii Zahorodnii</title>

        <!-- style -->
        <link rel="stylesheet" type="text/css" href="/style.css">
        <link rel="stylesheet" type="text/css" href="../style.css">

        <!-- Latest compiled and minified CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

        <link rel="icon" type="image/png" href="favicon/favicon-96x96.png" sizes="96x96" />
        <link rel="icon" type="image/svg+xml" href="favicon/favicon.svg" />
        <link rel="shortcut icon" href="favicon/favicon.ico" />
        <link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png" />
        <link rel="manifest" href="favicon/site.webmanifest" />
    </head>
    <body style="padding-bottom: 10px">
    <div class="container">
    <h1 class="col-md-offset-1">Andrii Zahorodnii</h1>
    <p class="col-md-offset-1"> zaho [at] mit [dot] edu </p>
    <!--- <p class="lead">undergrad studying cs and math<br> </p> -->
    <ul class="col-md-offset-1">
      <li><a href="/">Home</a></li> 
      <li> <a href="https://github.com/azaho" target="_blank">GitHub</a></li>
      <li> <a href="https://www.linkedin.com/in/zaho/" target="_blank">LinkedIn</a></li> 
      <li> <a href="https://scholar.google.com/citations?user=je-fgs8AAAAJ&hl=en" target="_blank">Google Scholar</a></li>
      <li> <a href="/#writing">Writing</a></li>
    </ul>
    </div><!-- /.container -->

    <div class="container" id="writing">
        <h2 class="col-md-offset-1">Research Proposal for Precision Neuroscience</h2>
        <hr>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
<p><i>I do research with Profs. Ila Fiete at MIT and Josh Aronson at BIDMC/Harvard Med School, developing self-supervised feature extraction models for human ECoG, with applications to intraoperative mapping and BCI. I'd love to advance this agenda working with Precision devices. Please feel free to contact me at zaho [at] mit [dot] edu if you have any questions. </i></p>
<hr>

<p>
    <b>Proposal 1: Representation learning from micro ECoG data for improved decoding and model-based stimulation.</b>
    <br/>
    • Ila Fiete and I have just <a href="https://azaho.org/papers/NeurIPS_2025__Contrastive_Neural_Forecasting_paper.pdf">submitted a paper to NeurIPS 2025</a> on a new self-supervised pretraining framework for foundation models for human ECoG, and its downstream applications to the <a href="https://azaho.org/papers/NeurIPS_2025__BTBench_paper.pdf">multimodal decoding benchmark I developed</a>.
    <br/>
    • As a first step, I propose to plug in the data from Precision cases into the same pipeline and assess any improvements in decoding accuracy (need to create the decoding benchmark first -- will do this).
    <img src="/precision/CNF_paper_preview.jpg">
</p>
<hr>
<p>
<b>Proposal 2: Model-enabled, high-resolution intraoperative mapping with Layer 7.</b>
<br/>
• Accurate intraoperative mapping is crucial, but current manual protocols are not scalable to high channel counts.
<br/>
• What if: the neurologist classifies only 20-30 LFP channels (e.g. normal / abnormal spiking), then we use a pretrained foundation model to extend those labels across all 1000+ channels in real time. The surgeon can freely move the device on the brain and interactively observe the classification boundary.
<br/>
• The labels still come intraoperatively from a neurologist. This is why I believe this will work.
<img src="/precision/proposal.jpg">
</p>

<p>
<b>Validation of the model on human SEEG.</b> From first principles, I proposed and created a foundation model architecture, pretrained it on autoregressive LFP prediction with no labels, and achieved promising preliminary results on a publicly available human SEEG dataset: the BrainTreebank, where subjects watched naturalistic movies. The resulting model enabled stronger decoding of speech onset events than the raw voltage inputs.
<br/><br/>
<p>
<img src="/precision/decoding_accuracy.jpg">
</p>
<b>Model details:</b>
<br/>
• Causal transformer, 5 layers, d_model=128, 4 attention heads per block
<br/>
• Sampling rate=2048 Hz
<br/>
• Trained for 1000 steps with batch size 128 with the contrastive loss (as in Figure 1)
<br/>
• Using the Muon optimizer, lr=0.003, no weight decay
</p>


            </div>
        </div>
    </div>

    <div class="container publications" style="margin-top: 20px;">
        <h2 class="col-md-offset-1">Projects & Publications</h2>
        <hr>
        <!--<hr>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
                *= equal contribution
            </div>
        </div>-->
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
            <div class="publication-container" style="display: flex; flex-direction: column; gap: 20px;">
                <div class="video-wrapper" style="max-width: 300px;">
                    <div class="video-container" style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
                        <img style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); width: auto; height: auto; max-width: 100%; max-height: 100%; object-fit: contain;" src="/papers/2025_bfm_intro_public_image2.jpg" alt="BT-bench visualization">
                    </div>
                </div>
                <div class="publication-content">
                    <h4>
                        Ongoing Research: Learning Representations of Human Neural Activity via Contrastive Neural Forecasting
                    </h4>
                    <h4>
                        [<a href="/papers/2025_bfm_intro_public.pdf">
                        project overview
                        </a>]
                        [<a href="/papers/NeurIPS_2025__Contrastive_Neural_Forecasting_paper.pdf">
                        paper
                        </a>]
                    </h4>
                    <p>Andrii Zahorodnii · Ila Fiete <i>in preparation (2025)</i></p>
                </div>
            </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
            <hr>
            <div class="publication-container" style="display: flex; flex-direction: column; gap: 20px;">
                <div class="video-wrapper" style="max-width: 300px;">
                    <div class="video-container" style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
                        <img style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); width: auto; height: auto; max-width: 100%; max-height: 100%; object-fit: contain;" src="/papers/2025_BTBench_image.jpg" alt="BT-bench visualization">
                    </div>
                </div>
                <div class="publication-content">
                    <h4>
                        Neuroprobe: Evaluating Intracranial Brain Responses to Naturalistic Stimuli
                    </h4>
                    <h4>
                        [<a href="/papers/NeurIPS_2025__BTBench_paper.pdf">
                        paper
                        </a>]
                        [<a href="https://github.com/azaho/neuroprobe/">
                        GitHub
                        </a>]
                        [<a href="https://neuroprobe.dev/">
                        Leaderboard Website
                        </a>]

                    </h4>
                    <p>Andrii Zahorodnii* · Bennett Stankovits* · Christopher Wang* · Charikleia Moraitaki · Geeling Chau · Ila R. Fiete · Boris Katz · Andrei Barbu <i>in preparation (2025)</i></p>
                </div>
            </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
            <hr>
            <div class="publication-container" style="display: flex; flex-direction: column; gap: 20px;">
                <div class="video-wrapper" style="max-width: 300px;">
                    <div class="video-container" style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
                        <video style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" autoplay muted loop playsinline>
                            <source src="https://d3vij5suwvbj3r.cloudfront.net/jenkins/jenkins_intro_vid.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>
                <div class="publication-content">
                    <h4>
                    Project Jenkins: Monkey Thinks, Robot Moves... and Back?
                    </h4>
                    <h4>
                        [<a href="https://www.808robots.com/projects/jenkins">
                        project page
                        </a>]
                        [<a href="https://github.com/azaho/jenkins">
                        code
                        </a>]
                    </h4>
                    <p>Andrii Zahorodnii · Dima Yanovsky <i>at MIT (2025)</i></p>
                </div>
            </div>
            <style>
                @media (min-width: 768px) {
                    .publication-container {
                        flex-direction: row !important;
                    }
                    .video-wrapper {
                        flex: 0 0 300px;
                    }
                    .publication-content {
                        flex: 1;
                    }
                }
            </style>
            </div>
        </div>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
            <hr>
            <h4>
            Overcoming sensory-memory interference in working memory circuits
            </h4>
            <h4>
                [<a href="https://www.biorxiv.org/content/10.1101/2025.03.17.643652v1.abstract">
                paper
                </a>]
            </h4>
            <p>Andrii Zahorodnii · Diego Mendoza-Halliday · Julio C. Martinez-Trujillo · Ning Qian · Robert Desimone · Christopher J. Cueva <i>on bioRxiv (2025)</i></p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
            <hr>
            <h4>
            Improving World Models using Deep Supervision with Linear Probes
            </h4>
            <h4>
                [<a href="https://arxiv.org/abs/2504.03861">
                paper
                </a>]
            </h4>
            <p>Andrii Zahorodnii <i>in ICLR 2025 Workshop on World Models, and on arXiv (2025)</i></p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
            <hr>
            <h4>
            Paper Quality Assessment based on Individual Wisdom Metrics from Open Peer Review
            </h4>
            <h4>
                [<a href="https://arxiv.org/abs/2501.13014">
                paper
                </a>]
            </h4>
            <p>Andrii Zahorodnii · Jasper J.F. van den Bosch · Ian Charest · Christopher Summerfield · Ila R. Fiete <i>on arXiv (2025)</i></p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
            <hr>
            <h4>
            The engagement of the cerebellum and basal ganglia enhances expertise in a sensorimotor adaptation task
            </h4>
            <h4>
            [<a href="https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00271/123895/The-engagement-of-the-cerebellum-and-basal-ganglia">
            paper
            </a>]
            </h4>
            <p>Joshua B Tan · Eli Müller · Andrii Zahorodnii · James M Shine <i>in Imaging Neuroscience, Vol. 2, pp. 1-20, MIT Press (2024)</i></p>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="col-md-offset-1">
            <hr>
            <span style="color: gray;">This website template was inspired by that of Christopher Wang at MIT.</span>
        </div>
    </div>
    </body>
</html>
